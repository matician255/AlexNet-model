So, in this paper the authors trained a large deep convolutional neural network to classify 1.2 million high image resolution in the ImageNet LSVRC 2012 contest, they were able to achieve a top-1 and top-5 error rates of 37.5% and 17% respectively in the test data, the network contained 60 million parameters and 650,000 neurons. The network is made of 5 convolutional networks, some containing max pool layers and 3 fully connected layers having a final SoftMax output layer with 1000 outputs, non-saturating neurons were used together with efficient GPU implementation of the convolution to make the model train faster. To reduce overfitting in the fully connected layers ‘dropout’ was used as regularization method.
The datasets were of different size and pixel values, since the model required images of same size, images were resized to 256x256 and also the average pixel value was subtracted from each pixel in each image thus centering the data making it easier for the model to learn faster. So, the 256x256 is center cropped to 224x224 before being fed to the first layer.
One of the novel features described in the paper is the rectified linear units (ReLu), in older neural networks the output of neurons was often calculated by function like tanh or sigmoid which are known as saturating nonlinearities because they flatten out(saturate) for very high or very low inputs thus make training slow because when the output saturates the gradients become very small thus the updates to the weights are tiny.
Therefore, they used ReLus which was much simpler where when the input is positive then output = input and when input is negative then output = 0, ReLus are non-saturating means they don’t flatten thus train much faster. The paper demonstrates an experiment proving that deep CNN with ReLu could reach 25% training error on CIFAR-10 dataset much faster than a network using tanh neurons.
Even though ReLu don’t require normalization, in this paper a special kind of normalization was used called Local Response Normalization to help the model generalize better and reduce error but to also it’s like creating competition like the brain neurons how they inhibit each other.
Don’t get me wrong on some instance’s tanh may perform better, it is just for this paper the problem was to prevent overfitting and help the model train faster which makes ReLu a better choice.
Another feature is the use of overlapping pooling which helps the model memorize the training data rather than learning general patterns.
